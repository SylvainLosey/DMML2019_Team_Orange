{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Orange: IMDB reviews sentiment analysis\n",
    "\n",
    "The purpose of the project is to build a model that would be able to classify the sentiment of a review of a movie and label if it has a positive or a negative sentiment. This way the opinions could be classified to be either negative or positive, while the medium sentiment is excluded from the analysis. \n",
    "Thus, the goal is to assign and weight either a positive or a negative connotation associated with each word or group of words. In addition, no weight is intended to be assigned to the words that are commonly used is sentence formation and that do not reflect any emotion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install all required dependencies for the future analysis in the current Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached https://files.pythonhosted.org/packages/9d/10/1dd2e3436e13402cc2b16c61b5f7407fb2e8057dcc18461db0d8e3523202/scikit_learn-0.22-cp37-cp37m-win_amd64.whl\n",
      "Collecting spacy\n",
      "  Using cached https://files.pythonhosted.org/packages/69/d8/f3103202aeca6fb0d2dbdd3a4ab1a7b86e9ad1d3cf8b23fa46bd466d64ac/spacy-2.2.3-cp37-cp37m-win_amd64.whl\n",
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/02/d0/1e8e60e61e748338e3a40e42f5dfeee63ccdecfc4f0894122b890bfb009a/pandas-0.25.3-cp37-cp37m-win_amd64.whl\n",
      "Collecting seaborn\n",
      "  Using cached https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl\n",
      "Collecting sklearn\n",
      "  Using cached https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting nltk\n",
      "  Using cached https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip\n",
      "Requirement already up-to-date: jupyter in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (1.0.0)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached https://files.pythonhosted.org/packages/8f/42/155696f85f344c066e17af287359c9786b436b1bf86029bb3411283274f3/joblib-0.14.0-py2.py3-none-any.whl\n",
      "Collecting scipy>=0.17.0\n",
      "  Using cached https://files.pythonhosted.org/packages/61/61/c81a5f4269c59cab509855d7690e81d36429dbbe104a4a631eef4736574f/scipy-1.3.3-cp37-cp37m-win_amd64.whl\n",
      "Collecting numpy>=1.11.0\n",
      "  Using cached https://files.pythonhosted.org/packages/34/40/c6eae19892551ff91bdb15f884fef2d42d6f58da55ab18fa540851b48a32/numpy-1.17.4-cp37-cp37m-win_amd64.whl\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached https://files.pythonhosted.org/packages/3c/5a/0d1b575ed40989d74fab25723083837c220246b25f3582917135cb32453f/preshed-3.0.2-cp37-cp37m-win_amd64.whl\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Using cached https://files.pythonhosted.org/packages/d5/7e/1981d5389b75543f950026de40a9d346e2aec7e860b2800e54e65bd46c06/blis-0.4.1-cp37-cp37m-win_amd64.whl\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/d5/46ff975f0d7d055cf95557b944fd5d29d9dfb37a4341038e070f212b24fe/catalogue-0.0.8-py2.py3-none-any.whl\n",
      "Collecting srsly<1.1.0,>=0.1.0\n",
      "  Using cached https://files.pythonhosted.org/packages/b7/a7/8829e71f434e442564dd9958299aa15c35885923aabe41bdc35f79887bad/srsly-0.2.0-cp37-cp37m-win_amd64.whl\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Using cached https://files.pythonhosted.org/packages/ff/ef/e8266e158ed32bf5f723fac862b6518833d0b53ca183165a8718f212c0d5/wasabi-0.4.2-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy) (41.4.0)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Using cached https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/7b/d77bc9bb101e113884b2d70a118e7ec8dcc9846a35a0e10d47ca37acdcbf/murmurhash-1.0.2-cp37-cp37m-win_amd64.whl\n",
      "Collecting thinc<7.4.0,>=7.3.0\n",
      "  Using cached https://files.pythonhosted.org/packages/9e/ed/7edded74724747f7dfc513f85b483db7828e4a1ed072c9625188dcb633a5/thinc-7.3.1-cp37-cp37m-win_amd64.whl\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy) (2.22.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached https://files.pythonhosted.org/packages/84/d1/35eab0c8cc9fd9432becaf3e90144762b3201a45079e62c47a8ae8739763/cymem-2.0.3-cp37-cp37m-win_amd64.whl\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Collecting pytz>=2017.2\n",
      "  Using cached https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl\n",
      "Collecting matplotlib>=1.4.3\n",
      "  Using cached https://files.pythonhosted.org/packages/dd/73/dc25ca27a9960539ef984921b0d42368445b856ae0861c3acba542b9a39c/matplotlib-3.1.2-cp37-cp37m-win_amd64.whl\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: qtconsole in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from jupyter) (4.6.0)\n",
      "Requirement already satisfied, skipping upgrade: nbconvert in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from jupyter) (5.6.1)\n",
      "Requirement already satisfied, skipping upgrade: jupyter-console in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from jupyter) (5.2.0)\n",
      "Requirement already satisfied, skipping upgrade: notebook in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from jupyter) (6.0.2)\n",
      "Requirement already satisfied, skipping upgrade: ipywidgets in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from jupyter) (7.5.1)\n",
      "Requirement already satisfied, skipping upgrade: ipykernel in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from jupyter) (5.1.3)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.36.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.2)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/0c/fc2e007d9a992d997f04a80125b0f183da7fb554f1de701bbb70a8e7d479/pyparsing-2.4.5-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached https://files.pythonhosted.org/packages/c6/ea/e5474014a13ab2dcb5056608e0716c600c3d8a8bcffb10ed55ccd6a42eb0/kiwisolver-1.1.0-cp37-none-win_amd64.whl\n",
      "Collecting cycler>=0.10\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: pygments in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from qtconsole->jupyter) (2.5.2)\n",
      "Requirement already satisfied, skipping upgrade: traitlets in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from qtconsole->jupyter) (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: jupyter-client>=4.1 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from qtconsole->jupyter) (5.3.4)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from qtconsole->jupyter) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: jupyter-core in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from qtconsole->jupyter) (4.6.1)\n",
      "Requirement already satisfied, skipping upgrade: nbformat>=4.4 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: testpath in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: jinja2>=2.4 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (2.10.3)\n",
      "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied, skipping upgrade: bleach in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (1.4.2)\n",
      "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (0.3)\n",
      "Requirement already satisfied, skipping upgrade: defusedxml in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: ipython in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from jupyter-console->jupyter) (7.10.1)\n",
      "Collecting prompt-toolkit<2.0.0,>=1.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/64/27/5fd61a451d086ad4aa806dc72fe1383d2bc0e74323668672287f616d5d51/prompt_toolkit-1.0.18-py3-none-any.whl (245kB)\n",
      "Requirement already satisfied, skipping upgrade: tornado>=5.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from notebook->jupyter) (6.0.3)\n",
      "Requirement already satisfied, skipping upgrade: Send2Trash in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from notebook->jupyter) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: terminado>=0.8.1 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from notebook->jupyter) (0.8.3)\n",
      "Requirement already satisfied, skipping upgrade: pyzmq>=17 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from notebook->jupyter) (18.1.0)\n",
      "Requirement already satisfied, skipping upgrade: prometheus-client in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from notebook->jupyter) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from ipywidgets->jupyter) (3.5.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from traitlets->qtconsole->jupyter) (4.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pywin32>=1.0; sys_platform == \"win32\" in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from jupyter-client>=4.1->qtconsole->jupyter) (223)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from nbformat>=4.4->nbconvert->jupyter) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from jinja2>=2.4->nbconvert->jupyter) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: webencodings in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: backcall in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from ipython->jupyter-console->jupyter) (0.1.0)\n",
      "Requirement already satisfied, skipping upgrade: colorama; sys_platform == \"win32\" in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from ipython->jupyter-console->jupyter) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from ipython->jupyter-console->jupyter) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: jedi>=0.10 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from ipython->jupyter-console->jupyter) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter) (0.1.7)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (7.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.15.6)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: parso>=0.5.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from jedi>=0.10->ipython->jupyter-console->jupyter) (0.5.1)\n",
      "Building wheels for collected packages: sklearn, nltk\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1321 sha256=1e1ecfadda7147294cd0ab7633526251464186db4bd0506820684ce036a84d6c\n",
      "  Stored in directory: C:\\Users\\Sylvain\\AppData\\Local\\pip\\Cache\\wheels\\76\\03\\bb\\589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449911 sha256=e96bef6fb1454a4308c03aba793c85b2f33c85b2abd0d958213edffeef28c67b\n",
      "  Stored in directory: C:\\Users\\Sylvain\\AppData\\Local\\pip\\Cache\\wheels\\96\\86\\f6\\68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built sklearn nltk\n",
      "Installing collected packages: joblib, numpy, scipy, scikit-learn, murmurhash, cymem, preshed, blis, catalogue, srsly, wasabi, plac, thinc, spacy, pytz, pandas, pyparsing, kiwisolver, cycler, matplotlib, seaborn, sklearn, nltk, prompt-toolkit\n",
      "  Found existing installation: prompt-toolkit 3.0.2\n",
      "    Uninstalling prompt-toolkit-3.0.2:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.2\n",
      "Successfully installed blis-0.4.1 catalogue-0.0.8 cycler-0.10.0 cymem-2.0.3 joblib-0.14.0 kiwisolver-1.1.0 matplotlib-3.1.2 murmurhash-1.0.2 nltk-3.4.5 numpy-1.17.4 pandas-0.25.3 plac-1.1.3 preshed-3.0.2 prompt-toolkit-1.0.18 pyparsing-2.4.5 pytz-2019.3 scikit-learn-0.22 scipy-1.3.3 seaborn-0.9.0 sklearn-0.0 spacy-2.2.3 srsly-0.2.0 thinc-7.3.1 wasabi-0.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: ipython 7.10.1 has requirement prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.18 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0MB)\n",
      "Requirement already satisfied: spacy>=2.2.2 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.17.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (41.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.0.8)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.2.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.36.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\sylvain\\miniconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (7.2.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py): started\n",
      "  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.5-cp37-none-any.whl size=12011744 sha256=fd9a3007d271e8935702257124ddd6875cab4251adbb08d993739667317c0e81\n",
      "  Stored in directory: C:\\Users\\Sylvain\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-h17z5d8q\\wheels\\6a\\47\\fb\\6b5a0b8906d8e8779246c67d4658fd8a544d4a03a75520197a\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.5\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U scikit-learn spacy pandas seaborn sklearn nltk jupyter\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy\n",
    "import nltk.corpus\n",
    "\n",
    "from nltk.corpus import words\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")\n",
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/imdb_dataset.csv\") \n",
    "\n",
    "# Keep the first 100 elements to reduce the load on cpu\n",
    "#data=data[:10]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "#### Text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def to_lower(this_review):\n",
    "    this_review=this_review.lower()\n",
    "    return this_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove HTML elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def remove_html(review):\n",
    "    return REMOVE_HTML.sub(\" \", review) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify and remove entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entities(this_review):\n",
    "    doc=sp(this_review)\n",
    "    \n",
    "    for i in doc.ents:\n",
    "            i=str(i)\n",
    "            this_review=this_review.replace(\" \"+i,\"\")\n",
    "    return this_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing lemmatization\n",
    "def lemmatize_it(this_review):\n",
    "    filtered_sent=[]\n",
    "\n",
    "    #  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "    lem = sp(this_review)\n",
    "    \n",
    "   # finding lemma for each word\n",
    "    for word in lem:\n",
    "        filtered_sent.append(word.lemma_)\n",
    "    return filtered_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "nlp = spacy.lang.en.English()\n",
    "\n",
    "def tokenize_review(this_review):\n",
    "    my_doc = nlp(this_review)\n",
    "    \n",
    "    # Create list of word tokens\n",
    "    token_list = []\n",
    "    for token in my_doc:\n",
    "        token_list.append(token.text)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapt spacy stopwords list to our topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print stopword list from spacy\n",
    "spacy_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "\n",
    "remove_from_stopwordlist=[\"n't\", \"most\", \"much\", \"never\", \"no\", \"not\", \"nothing\", \"n‘t\", \"n’t\", \"really\", \"top\", \"very\", \"well\"]\n",
    "for word in spacy_stopwords:\n",
    "    if word in remove_from_stopwordlist:\n",
    "         spacy_stopwords.remove(word)\n",
    "\n",
    "add_to_stopwords=['.', ',', '!', '?', ':', '&', '...', '(', ')','-', '/', '\"', ';', '-PRON-', ' ', \"'\", '....', '  ', '*']\n",
    "for word in add_to_stopwords:\n",
    "    spacy_stopwords.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Remove stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_stopwords(this_review):\n",
    "    \n",
    "    filtered_sent=[]\n",
    "\n",
    "    #  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "    doc = this_review\n",
    "    \n",
    "    # filtering stop words\n",
    "    for word in doc:\n",
    "        if word not in spacy_stopwords:\n",
    "            filtered_sent.append(word)\n",
    "    return filtered_sent\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if tokens are words and remove those that aren't (using NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/sylvain/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "def remove_non_words(this_review):\n",
    "    clean_sent=[]\n",
    "    for word in this_review:\n",
    "        if word in words.words():\n",
    "            clean_sent.append(word)\n",
    "    return clean_sent        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reconcatenate list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcatenate_list_to_string (this_review):\n",
    "    this_review=' '.join(this_review)\n",
    "    return this_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(this_review):\n",
    "    this_review=to_lower(this_review)\n",
    "    this_review=remove_html(this_review)\n",
    "    this_review=remove_entities(this_review)\n",
    "    this_review=lemmatize_it(this_review)\n",
    "    this_review=eliminate_stopwords(this_review)\n",
    "    # Commented out because it uses a LOT of time\n",
    "    # this_review=remove_non_words(this_review)\n",
    "    this_review=reconcatenate_list_to_string (this_review)\n",
    "    \n",
    "    return this_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [1:16:32<00:00, 10.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reviewer mention watch oz episode hook right e...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>think wonderful way spend time hot sit air con...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter love time money visually stunning film ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  reviewer mention watch oz episode hook right e...  positive\n",
       "1  wonderful little production filming technique ...  positive\n",
       "2  think wonderful way spend time hot sit air con...  positive\n",
       "3  basically family little boy jake think zombie ...  negative\n",
       "4  petter love time money visually stunning film ...  positive"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "data['review'] = data['review'].progress_map(cleaning)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "\n",
    "data.to_csv(\"../data/super_clean_dataset.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.4"
=======
   "version": "3.7.3"
>>>>>>> 6708234cd32271ed4e51edc4ec815d088cf8d4b0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
